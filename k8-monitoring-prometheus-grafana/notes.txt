Monitoring:
Monitoring involves collecting and analyzing data from your systems (like CPU usage, memory usage, network traffic, etc.) to detect and alert on predefined conditions or thresholds. It helps you know when something is wrong with your system.
In DevOps, monitoring focuses on:

Tracking system metrics (e.g., CPU, RAM, disk usage)
Health checks of services and containers
Setting alerts when metrics exceed certain thresholds
Log monitoring for error patterns or performance bottlenecks
Uptime/downtime of services


Observability:(LMT,logs,metrics,tracing)
Observability is a more holistic approach that goes beyond monitoring. It involves understanding the internal states of a system by analyzing the outputs (logs, metrics, traces) to gain insights into the system’s behavior. The goal is not only to detect problems but also to understand why they occur, even without predefined conditions.

In DevOps, observability focuses on:

Logs: Collecting and analyzing logs from different components
Metrics: Custom and system-wide performance metrics
Tracing: End-to-end tracing of requests across distributed services
Correlating data from various sources to diagnose issues

In Kubernetes, a custom metrics server allows you to collect application-specific metrics that can be used for autoscaling or monitoring. One popular example is Prometheus Adapter, which integrates Prometheus metrics with Kubernetes' Horizontal Pod Autoscaler (HPA) using custom metrics.

install Prometheus Adapter using helm 

You need to create custom metrics in your application and ensure Prometheus scrapes those metrics. For example, if your app exposes metrics at /metrics endpoint, Prometheus will collect them.
http_requests_total{app="myapp"} 150
http_requests_total{app="myapp"} 150
efficiently.


Kubernetes API Server (kube-apiserver)
The Kubernetes API server is the central component of the Kubernetes control plane.
It acts as the interface through which all communication happens in the cluster. The API server handles requests from various clients (kubectl, K8s controllers, or external services like Prometheus) and interacts with etcd (the key-value store that keeps the state of the cluster).
It provides access to information about all the resources (pods, services, nodes, events, etc.) in the cluster.

In a Kubernetes (K8s) cluster, the Kubernetes API server and Prometheus server work together in several important ways, primarily related to monitoring and gathering metrics from the cluster. Here's how they interact and function together:

Monitoring in Kubernetes:
Node and Pod Monitoring:

Metrics Server: Collects CPU and memory usage data from K8s nodes and pods.
Prometheus: The most popular monitoring tool for Kubernetes. It scrapes metrics from the Kubernetes API and other services, storing them in a time-series database. Prometheus is often paired with Grafana for visualizing metrics.
Alerting:

Prometheus’ Alertmanager can be used to set up rules to notify teams (e.g., Slack, email) if critical thresholds are exceeded.
Log Monitoring:

Fluentd, Promtail, or Logstash can be used to collect logs from Kubernetes pods and send them to a centralized logging system like ElasticSearch or Loki.


Service Mesh Observability:

If you're using a service mesh like Istio or Linkerd, these provide built-in observability features like automatic tracing, metrics collection, and traffic monitoring at the service level.

Tools in K8s for Monitoring and Observability:
Prometheus (metrics collection, alerting)
Grafana (visualization of metrics)
Fluentd, Loki, or ELK Stack (log aggregation and analysis)
Jaeger or OpenTelemetry (distributed tracing)
Kiali (observability in Istio service mesh)

Prometheus components:
=================
Prometheus server>>Scraping: Periodically pulls metrics from configured endpoints (targets).
Exporters collect metrics from various sources and expose them in a format that Prometheus can scrape.
alert manager>Handles alerts generated by the Prometheus server.
PromQL (Prometheus Query Language
Prometheus stores the time-series data on the local disk of the Kubernetes node where the Prometheus pod is running. The data is stored in a compressed format within the TSDB.
 Ephemeral Storage vs. Persistent Storage
If Prometheus is running without a persistent volume (using ephemeral storage), then the metrics will be lost if the pod is terminated, rescheduled, or if the node fails.
To ensure high availability and durability of the metrics data, it's recommended to use persistent storage by mounting a PV.
For long-term storage or high availability, Prometheus can be integrated with external storage systems (e.g., Thanos, Cortex, or M3DB) that allow data replication and durable storage beyond the local node’s d

By default, the Helm chart might configure ephemeral storage, meaning that Prometheus will store its data on the local node's filesystem without persistence. To enable PVC-based persistent storage, you would modify the values file.

If you don’t configure a PVC, Prometheus will store data on the ephemeral storage of the Kubernetes node where the pod is running.
If the pod is restarted, rescheduled, or the node goes down, all the metrics data will be lost. This is why PVCs are highly recommended in production setups where data persistence is crucial.

Prometheus is usually deployed as a StatefulSet in Kubernetes when persistent storage is required. StatefulSets ensure that pods are rescheduled with stable network identities and volumes, preserving the data even after restarts.
A Deployment can be used if you are okay with ephemeral storage (non-persistent).

If the Prometheus pod gets restarted, rescheduled, or if the node hosting the Prometheus pod goes down, then all the metrics data stored on ephemeral storage will be lost. This is why PVCs are used to provide persistent storage for Prometheus in production environments.
kube-prometheus stack



How Prometheus Server and Kubernetes API Server Work Together
A. Service Discovery of Kubernetes Targets
One of the main ways Prometheus interacts with the Kubernetes API server is through service discovery.
Prometheus queries the Kubernetes API server to dynamically discover and scrape metrics from various components in the cluster. This eliminates the need to manually configure individual endpoints for scraping.
The Prometheus server uses the API server to:
Discover all running pods, services, and nodes.
Scrape metrics from the Kubernetes components (like kubelet, kube-scheduler, kube-controller-manager) and application pods exposing metrics endpoints.

Prometheus typically scrapes the following:

Node Exporter: Metrics about node-level resources (CPU, memory, disk usage, etc.).
kube-state-metrics: A tool that generates metrics based on the current state of Kubernetes resources as reported by the API server (e.g., the status of pods, deployments, daemonsets, etc.).
cAdvisor: Container metrics like CPU, memory, and network statistics for running containers.
Custom Application Metrics: Metrics exposed by the pods running inside the cluster (e.g., your application).

The API server metrics endpoint is typically exposed at /metrics, which can be scraped by Prometheus

Kubernetes generates events for different changes or issues in the cluster (e.g., pod failures, node issues, etc.). These events are accessible via the API server.

Prometheus Operator
The Prometheus Operator simplifies managing Prometheus in Kubernetes by automating many tasks related to deployment, configuration, and lifecycle management of Prometheus.
The Prometheus Operator interacts with the Kubernetes API server to manage Prometheus instances as custom resources (using the Prometheus custom resource definition or CRD).
It uses the Kubernetes API to dynamically create and manage Prometheus instances, Alertmanager, and ServiceMonitors based on user-defined configurations.
The ServiceMonitor CRD allows users to define which services should be monitored by Prometheus, and the Operator configures Prometheus to scrape the necessary metrics automatically. The Prometheus Operator relies heavily on the Kubernetes API server for this orchestration


Prometheus can alert on the state of Kubernetes resources by querying the Kubernetes API for resource conditions or events. For example, Prometheus can monitor pod restarts, node status, or pending deployments by accessing data exposed by the Kubernetes API and sending alerts based on this information.

important:
A ServiceMonitor is a Custom Resource Definition (CRD) in Prometheus Operator that allows you to configure how Prometheus discovers and scrapes services in Kubernetes. It simplifies the process of monitoring Kubernetes services by enabling Prometheus to automatically find and scrape the services based on labels without manually adding scrape configurations.
Workflow:
Create a ServiceMonitor CRD: Define the ServiceMonitor resource in Kubernetes.
Prometheus Operator watches: The Prometheus Operator watches for changes to ServiceMonitor objects.
Prometheus automatically scrapes: The Operator automatically adds matching services to Prometheus’ scrape configuration based on the ServiceMonitor specificatio
In Kubernetes, services (and pods) are usually labeled with metadata (e.g., app=my-app, tier=backend). These labels are flexible and can be used to organize and filter resources.
Label matching: In the ServiceMonitor, you specify which labels to look for (like app=my-app). Prometheus uses these labels to match the relevant services. For example, Prometheus will automatically scrape any service that has the label app=my-app without you having to manually list each service.
Normally, in Prometheus, you would define a scrape configuration manually, telling Prometheus which targets to scrape (e.g., specific IPs, ports, paths, etc.). This would require constant updates if services change or new services are added.
With ServiceMonitor, you don’t need to manually configure each service in Prometheus. By simply defining the ServiceMonitor with the right label selectors and endpoints, Prometheus will know which services to scrape based on the labels that match


The Kubernetes API server serves as a source of information about the state of the cluster, providing details about the nodes, pods, services, and events.
For Kubernetes, Prometheus uses the Kubernetes service discovery mechanism by querying the Kubernetes API server to find and monitor running pods, services, nodes, or endpoints that expose metrics.
This is done using kubernetes_sd_configs (Kubernetes service discovery configurations) in the Prometheus configuration file (prometheus.yml).

In the Prometheus configuration (prometheus.yml), you can specify a role of pod under the kubernetes_sd_configs section. This tells Prometheus to look for and scrape metrics from all the pods in the cluster.

important
===========
Example Use Case: Scraping Metrics from a Node.js or Python Application
Suppose you have a Node.js or Python application running inside a pod, and this application exposes metrics at the /metrics endpoint (using a library like prom-client in Node.js or prometheus_client in Python).

These applications typically expose metrics related to:
Request counts
Response times (latency)
Error rates
Resource usage (CPU, memory)


How Does Pod Discovery Handle Changes:
Pod Lifecycle: If a pod is rescheduled or restarted, Prometheus will notice the change, discover the new pod, and begin scraping it.
Scaling Applications: When applications scale (e.g., more replicas are added), Prometheus automatically discovers the new pods and starts scraping them


important:
=========
How Prometheus Scrapes Metrics from Nginx:
Expose Nginx Metrics: Nginx does not expose Prometheus metrics natively out of the box. To expose Nginx metrics, you'll need to set up an exporter (typically the nginx-prometheus-exporter) that converts Nginx's internal statistics into a format that Prometheus can scrape.